name: Enrich headlines.json

on:
  push:
    branches: [ main ]
    paths:
      - 'headlines.json'
      - 'scripts/**'
      - '.github/workflows/enrich.yml'
  workflow_dispatch: {}  # allow manual runs from the Actions tab

jobs:
  enrich:
    runs-on: ubuntu-latest
    permissions:
      contents: write   # allow the workflow to commit back to the repo
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Write enrich_headlines.py (inline)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat > scripts/enrich_headlines.py <<'PY'
          #!/usr/bin/env python3
          # scripts/enrich_headlines.py
          #
          # Enrich + clean headlines.json:
          #   1) DROP aggregators (news.google.com, news.yahoo, apple.news, etc.)
          #   2) Fuzzy de-dupe (token signature) across outlets (keeps newest; prefer non-aggregators)
          #   3) Canonicalize URL + add canonical_id / cluster_id / paywall / opinion
          #
          # Usage:
          #   python scripts/enrich_headlines.py headlines.json --inplace
          #   python scripts/enrich_headlines.py headlines.json --out headlines.enriched.json

          import argparse
          import hashlib
          import json
          import re
          from datetime import datetime, timezone
          from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

          # ---------------- Settings ----------------

          TRACKING_PARAMS = {
              "utm_source","utm_medium","utm_campaign","utm_term","utm_content",
              "utm_name","utm_id","utm_reader","utm_cid",
              "fbclid","gclid","mc_cid","mc_eid","cmpid","s_kwcid","sscid",
              "ito","ref","smid","sref","partner","ICID","ns_campaign",
              "ns_mchannel","ns_source","ns_linkname","share_type","mbid"
          }

          PAYWALL_DOMAINS = {
              "ft.com","wsj.com","theglobeandmail.com","bloomberg.com","nytimes.com",
              "economist.com","latimes.com","thelogic.co","nationalpost.com","financialpost.com"
          }

          OPINION_TITLE_PAT = re.compile(r"\b(opinion|op-ed|analysis|commentary|column)\b", re.I)
          OPINION_PATH_PAT  = re.compile(r"/(opinion|commentary|analysis|column)s?/", re.I)

          # Fuzzy signature (mirrors client)
          STOPWORDS = {
              "the","a","an","and","or","but","of","for","with","without","in","on","at","to","from","by","as","into","over","under","than","about",
              "after","before","due","will","still","just","not","is","are","was","were","be","being","been","it","its","this","that","these","those",
              "live","update","breaking","video","photos","report","reports","says","say","said",
              "vs","vs.","game","games","preview","recap","season","start","starts","starting","lineup",
              "dead","killed","kills","kill","dies","die","injured","injures","injury",
              "los","angeles","new","york","la"
          }
          TOKEN_RE = re.compile(r"[a-z0-9]+", re.I)

          def strip_source_tail(title: str) -> str:
              return re.sub(r"\s+[-–—]\s+[^|]+$", "", title or "")

          def fuzzy_title_key(title: str) -> str:
              base = strip_source_tail(title or "").lower()
              toks = [t for t in TOKEN_RE.findall(base) if len(t) > 1 and t not in STOPWORDS]
              if not toks:
                  raw = TOKEN_RE.findall(base)
                  sig = "|".join(raw)[:200] or "empty"
                  return f"fk:{sig}"
              uniq = sorted(set(toks))
              sig = "|".join(uniq[:10])
              return f"fk:{sig}"

          def sha1_hex(s: str, n: int = 12) -> str:
              return hashlib.sha1(s.encode("utf-8")).hexdigest()[:n]

          def domain_of(url: str) -> str:
              try:
                  return (urlparse(url).netloc or "").lower()
              except Exception:
                  return ""

          def is_aggregator(url: str | None, source: str | None = None) -> bool:
              s = f"{source or ''} {url or ''}".lower()
              return any(k in s for k in ("news.google", "google news", "news.yahoo", "apple.news", "bing.com/news", "newsfeed"))

          def canonicalize_url(url: str) -> str:
              if not url:
                  return ""
              try:
                  u = urlparse(url)
                  scheme = "https"
                  netloc = (u.netloc or "").lower()

                  # strip common mobile subdomains
                  if netloc.startswith("m.") and "." in netloc[2:]:
                      netloc = netloc[2:]
                  elif netloc.startswith("mobile.") and "." in netloc[7:]:
                      netloc = netloc[7:]

                  path = u.path or "/"
                  # drop tracking params
                  q = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True) if k not in TRACKING_PARAMS]
                  query = urlencode(q, doseq=True)

                  if path != "/" and path.endswith("/"):
                      path = path[:-1]

                  return urlunparse((scheme, netloc, path, "", query, ""))
              except Exception:
                  return url

          def canonical_id_from_url(url: str) -> str:
              base = canonicalize_url(url)
              return f"u:{hashlib.sha1(base.encode('utf-8')).hexdigest()[:16]}"

          def looks_paywalled(url: str, source: str | None = None) -> bool:
              host = domain_of(url)
              if any(host.endswith(d) for d in PAYWALL_DOMAINS):
                  return True
              if source:
                  s = source.lower()
                  if any(k in s for k in ("wall street journal","financial times","globe and mail","bloomberg","new york times","economist","the logic","national post","financial post")):
                      return True
              return False

          def looks_opinion(url: str, title: str | None = None) -> bool:
              try:
                  if title and OPINION_TITLE_PAT.search(title):
                      return True
                  path = urlparse(url).path or ""
                  if OPINION_PATH_PAT.search(path):
                      return True
              except Exception:
                  pass
              return False

          def parse_ts(it: dict) -> float:
              v = it.get("published_utc") or it.get("published") or ""
              if not v:
                  return 0.0
              try:
                  # Accept 'Z' or +00:00
                  if v.endswith("Z"):
                      v = v.replace("Z", "+00:00")
                  return datetime.fromisoformat(v).timestamp()
              except Exception:
                  return 0.0

          def enrich_item(it: dict) -> dict:
              url = it.get("url","")
              title = it.get("title","")
              source = it.get("source","")

              can_url = canonicalize_url(url)
              can_id  = canonical_id_from_url(url)

              # cluster_id based on fuzzy signature (stable across outlets)
              fkey = fuzzy_title_key(title)
              clid = f"t:{sha1_hex(fkey, 12)}"

              it["canonical_url"] = can_url
              it["canonical_id"]  = can_id
              it["cluster_id"]    = clid
              it["paywall"]       = looks_paywalled(can_url, source)
              it["opinion"]       = looks_opinion(can_url, title)
              return it

          def dedupe_items(items: list[dict]) -> list[dict]:
              """Use fuzzy signature key. Keep newest; tiebreak to non-aggregator."""
              by = {}
              for it in items:
                  key = fuzzy_title_key(it.get("title",""))
                  prev = by.get(key)
                  if prev is None:
                      by[key] = it
                      continue
                  t_new = parse_ts(it)
                  t_old = parse_ts(prev)
                  if t_new > t_old:
                      by[key] = it
                  elif t_new == t_old:
                      if is_aggregator(prev.get("url"), prev.get("source")) and not is_aggregator(it.get("url"), it.get("source")):
                          by[key] = it
              return list(by.values())

          def main():
              ap = argparse.ArgumentParser(description="Enrich headlines.json with canonical IDs and flags, and remove duplicates.")
              ap.add_argument("input", help="Path to headlines.json")
              ap.add_argument("--out", help="Output path (default: print to stdout)")
              ap.add_argument("--inplace", action="store_true", help="Write back to the same file")
              args = ap.parse_args()

              with open(args.input, "r", encoding="utf-8") as f:
                  data = json.load(f)

              items = list(data.get("items", []))

              # 1) Drop aggregator mirrors completely
              items = [it for it in items if not is_aggregator(it.get("url"), it.get("source"))]

              # 2) De-dupe across outlets via fuzzy signature
              items = dedupe_items(items)

              # 3) Enrich remaining items
              enriched = [enrich_item(dict(it)) for it in items]

              out = dict(data)
              out["items"] = enriched
              out["count"] = len(enriched)
              out.setdefault("generated_utc", datetime.now(timezone.utc).isoformat())

              if args.inplace:
                  with open(args.input, "w", encoding="utf-8") as f:
                      json.dump(out, f, ensure_ascii=False, indent=2)
              elif args.out:
                  with open(args.out, "w", encoding="utf-8") as f:
                      json.dump(out, f, ensure_ascii=False, indent=2)
              else:
                  print(json.dumps(out, ensure_ascii=False, indent=2))

          if __name__ == "__main__":
              main()
          PY
          chmod +x scripts/enrich_headlines.py

      - name: Enrich headlines.json
        run: |
          python scripts/enrich_headlines.py headlines.json --inplace
          echo "---- Post-enrichment sanity ----"
          python - <<'PY'
          import json
          from urllib.parse import urlparse
          with open("headlines.json","r",encoding="utf-8") as f:
              d=json.load(f)
          items=d.get("items",[])
          print("items:",len(items))
          # Count aggregator leftovers (should be 0)
          aggr = [it for it in items if any(s in (it.get("url","")+it.get("source","")).lower() for s in ("news.google","news.yahoo","apple.news","bing.com/news"))]
          print("aggregators left:",len(aggr))
          # Show some cluster_ids sample
          print("sample cluster_ids:", [it.get("cluster_id") for it in items[:5]])
          PY

      - name: Commit enriched JSON (if changed)
        run: |
          if ! git diff --quiet -- headlines.json; then
            git config user.name  "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add headlines.json
            git commit -m "CI: filter aggregators + fuzzy de-dupe + canonicalize/paywall/opinion"
            git push
          else
            echo "No enrichment changes."
          fi
