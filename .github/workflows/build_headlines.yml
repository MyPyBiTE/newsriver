name: Build & publish headlines.json

on:
  workflow_dispatch:
  schedule:
    - cron: "*/12 * * * *"  # every 12 minutes

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    env:
      # Collector caps (collect plenty, then filter hard)
      MPB_MAX_PER_FEED: "14"
      MPB_MAX_TOTAL: "320"

      # Time + budgets
      MPB_HTTP_TIMEOUT: "10"
      MPB_SLOW_FEED_WARN: "3.5"
      MPB_GLOBAL_BUDGET: "210"
      NEWSRIVER_TIMEZONE: "America/Toronto"
      MPB_BREAKER_LIMIT: "3"

      # Recency + exact publish count (pre-clean)
      MPB_MAX_AGE_HOURS: "69"
      MPB_MIN_AGE_SEC: "60"
      MPB_REQUIRE_EXACT_COUNT: "69"

      # Strict link verification inside the Python collector
      MPB_VERIFY_LINKS: "1"
      MPB_REJECT_REDIRECT_TO_HOMEPAGE: "1"
      MPB_BLOCK_AGGREGATORS: "1"
      MPB_MIN_BODY_BYTES: "4096"
      MPB_MIN_ARTICLE_WORDS: "120"

      # Safety net for Python collector
      MPB_FALLBACK_MIN_ITEMS: "1"
      MPB_FALLBACK_MAX_AGE_HOURS: "24"

      # Realistic UA
      MPB_ALT_UA: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"

      # Cleaner thresholds (post-build)
      CLEAN_MAX_AGE_HOURS: "72"         # target clamp
      CLEAN_MIN_BODY_BYTES: "2800"      # softened
      CLEAN_MIN_WORDS: "80"             # softened
      CLEAN_CONCURRENCY: "6"
      CLEAN_TIMEOUT_MS: "12000"
      CLEAN_RELAX_THRESHOLD: "30"       # if strict < 30, do relaxed pass
      CLEAN_RELAX_MAX_AGE_HOURS: "96"   # relaxed clamp

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install tools (jq + curl)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests beautifulsoup4 json5

      - name: Guardrail — strip embedded README block if present
        shell: bash
        run: |
          set -euo pipefail
          if grep -n '^readme = """' scripts/fetch_headlines.py >/dev/null 2>&1; then
            echo ">> Stripping embedded README block from scripts/fetch_headlines.py"
            sed -i '/^readme = """/,$d' scripts/fetch_headlines.py
          fi

      - name: Syntax check
        run: python -m py_compile scripts/fetch_headlines.py

      - name: Build headlines.json
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p newsriver
          python scripts/fetch_headlines.py --feeds-file feeds.txt --out newsriver/headlines.json
          echo "----- basic check"
          jq -r '.generated_utc, .count' newsriver/headlines.json || true
          echo "----- sample URLs"
          jq -r '.items[:10][] | [.published_utc, .source, .title, .url] | @tsv' newsriver/headlines.json || true

      - name: Set up Node (for cleaner)
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Write cleaner script (to temp, not repo)
        shell: bash
        run: |
          set -euo pipefail
          CLEANER="$RUNNER_TEMP/validate-links.mjs"
          cat << 'EOF' > "$CLEANER"
          // Post-build cleaner for newsriver/headlines.json
          // Pass A (strict): ≤72h, follow redirects, 2xx only, body bytes/words, anti-aggregator, article-like URL
          // If < RELAX_THRESHOLD => Pass B (relaxed): ≤96h, skip body word/byte checks, keep anti-aggregator + article-like URL
          import fs from "node:fs";

          const DATA_PATH = "newsriver/headlines.json";

          const MAX_AGE_MS_STRICT = Number(process.env.CLEAN_MAX_AGE_HOURS || "72") * 3600 * 1000;
          const MAX_AGE_MS_RELAX  = Number(process.env.CLEAN_RELAX_MAX_AGE_HOURS || "96") * 3600 * 1000;
          const RELAX_THRESHOLD   = Number(process.env.CLEAN_RELAX_THRESHOLD || "30");

          const MIN_BYTES  = Number(process.env.CLEAN_MIN_BODY_BYTES || "2800");
          const MIN_WORDS  = Number(process.env.CLEAN_MIN_WORDS || "80");
          const CONC       = Number(process.env.CLEAN_CONCURRENCY || "6");
          const TIMEOUT_MS = Number(process.env.CLEAN_TIMEOUT_MS || "12000");
          const UA         = process.env.MPB_ALT_UA || "Mozilla/5.0 mypybite-cleaner";

          const AGGR = /(news\.google|news\.yahoo|apple\.news|bing\.com\/news|msn\.com\/en-|flipboard\.com|drudgereport\.com|newsnow\.co\.uk|feedly\.com)/i;
          const SECTION = /\/(news|world|business|markets|sports|technology|tech|culture|entertainment|opinion|politics|video|videos|live|frontpage|homepage|home|today)\/*$/i;
          const HOMEPAGE = /^\/?($|[#?])/;
          const SOFT_404_PAT = /(404|not\s+found|page\s+not\s+found|no\s+longer\s+available|removed|error\s+404)/i;

          function absoluteUrl(u) {
            if (!u) return null;
            if (/^\/\//.test(u)) return "https:" + u;
            if (/^[a-z0-9.-]+\.[a-z]{2,}\b(\/|$)/i.test(u)) return "https://" + u;
            if (/^https?:\/\//i.test(u)) return u;
            return null;
          }

          function normalize(u) {
            try {
              const url = new URL(u);
              url.hash = "";
              const skip = /^(utm_|fbclid$|gclid$|mc_(cid|eid)$|ref$|cmpid$|source$|scid$)/i;
              const kept = [...url.searchParams.entries()].filter(([k]) => !skip.test(k));
              kept.sort(([a],[b]) => a.localeCompare(b));
              url.search = "";
              kept.forEach(([k,v]) => url.searchParams.append(k,v));
              return url.toString().replace(/\/+$/,'');
            } catch {
              return u;
            }
          }

          function isLikelyArticle(u) {
            try {
              const url = new URL(u);
              const p = url.pathname;
              if (!/^https?:\/\//i.test(u)) return false;
              if (AGGR.test(u)) return false;
              if (HOMEPAGE.test(p) || SECTION.test(p)) return false;
              const segs = p.split("/").filter(Boolean);
              if (segs.length <= 1 && !/\d{4}/.test(p)) return false;
              if (!/[a-z]{3,}/i.test(decodeURIComponent(p))) return false;
              return true;
            } catch {
              return false;
            }
          }

          function within(tsIso, maxAgeMs) {
            const ts = Date.parse(tsIso || "");
            return Number.isFinite(ts) && (Date.now() - ts <= maxAgeMs);
          }

          async function fetchWithTimeout(url, opts = {}) {
            const ctl = new AbortController();
            const t = setTimeout(() => ctl.abort(), TIMEOUT_MS);
            try {
              const res = await fetch(url, {
                redirect: "follow",
                headers: { "user-agent": UA, "accept": "text/html,application/xhtml+xml" },
                signal: ctl.signal,
                ...opts,
              });
              clearTimeout(t);
              return res;
            } catch {
              clearTimeout(t);
              return { ok:false, status:0, url, text: async () => "" };
            }
          }

          function looksLikeArticleBody(html = "") {
            if (!html || html.length < MIN_BYTES) return false;
            if (SOFT_404_PAT.test(html)) return false;
            const text = html
              .replace(/<script[\s\S]*?<\/script>/gi," ")
              .replace(/<style[\s\S]*?<\/style>/gi," ")
              .replace(/<[^>]+>/g," ")
              .replace(/\s+/g," ")
              .trim();
            const words = text.split(" ").filter(Boolean);
            return words.length >= MIN_WORDS;
          }

          function loadJSON() {
            const raw = JSON.parse(fs.readFileSync(DATA_PATH, "utf8"));
            if (Array.isArray(raw)) return { kind:"array", items: raw, wrapper:null };
            if (Array.isArray(raw?.items)) return { kind:"object", items: raw.items, wrapper: raw };
            return { kind:"array", items:[], wrapper:null };
          }

          function saveJSON(kind, wrapper, items) {
            if (kind === "object") {
              const out = {
                ...wrapper,
                generated_utc: new Date().toISOString(),
                count: items.length,
                items
              };
              fs.writeFileSync(DATA_PATH, JSON.stringify(out, null, 2) + "\n","utf8");
            } else {
              fs.writeFileSync(DATA_PATH, JSON.stringify(items, null, 2) + "\n","utf8");
            }
          }

          function prep(items, maxAgeMs) {
            const seen = new Set();
            return items.map(it => {
              const u = absoluteUrl(it?.canonical_url || it?.url) || absoluteUrl(it?.url);
              if (!u) return null;
              return {
                ...it,
                url: normalize(u),
                published_utc: it?.published_utc || it?.published || it?.date || new Date().toISOString()
              };
            }).filter(Boolean)
              .filter(it => within(it.published_utc, maxAgeMs))
              .filter(it => isLikelyArticle(it.url))
              .filter(it => { if (seen.has(it.url)) return false; seen.add(it.url); return true; });
          }

          // simple concurrency limiter
          async function mapLimit(arr, limit, fn) {
            const out = new Array(arr.length);
            let i = 0, active = 0;
            return new Promise((resolve) => {
              const pump = () => {
                while (active < limit && i < arr.length) {
                  const idx = i++; active++;
                  Promise.resolve(fn(arr[idx], idx))
                    .then(v => { out[idx] = v; })
                    .catch(() => { out[idx] = null; })
                    .finally(() => { active--; pump(); });
                }
                if (i >= arr.length && active === 0) resolve(out);
              };
              pump();
            });
          }

          async function strictClean(base) {
            return (await mapLimit(base, CONC, async (it) => {
              try {
                let res = await fetchWithTimeout(it.url, { method: "HEAD" });
                if (!res.ok) res = await fetchWithTimeout(it.url, { method: "GET" });
                if (!res.ok || res.status < 200 || res.status >= 300) return null;

                const finalUrl = normalize(res.url || it.url);
                if (!isLikelyArticle(finalUrl)) return null;

                const res2 = await fetchWithTimeout(finalUrl, { method: "GET" });
                if (!res2.ok) return null;
                const body = await res2.text();
                if (!looksLikeArticleBody(body)) return null;

                const keep = {
                  title: String(it.title || "").trim(),
                  url: finalUrl,
                  source: it.source || "",
                  published_utc: new Date(it.published_utc).toISOString()
                };
                if (it.effects) keep.effects = it.effects;
                if (it.category) keep.category = it.category;
                if (it.region) keep.region = it.region;
                return keep;
              } catch {
                return null;
              }
            })).filter(Boolean);
          }

          (async function main(){
            const { kind, wrapper, items } = loadJSON();

            // Pass A — strict
            const baseStrict = prep(items, MAX_AGE_MS_STRICT);
            let cleaned = await strictClean(baseStrict);

            // Pass B — relaxed fallback (only if too few survive)
            if (cleaned.length < RELAX_THRESHOLD) {
              const baseRelax = prep(items, MAX_AGE_MS_RELAX);
              // relaxed: skip body/bytes checks; just confirm 2xx and article-like
              const relaxed = (await mapLimit(baseRelax, CONC, async (it) => {
                try {
                  let res = await fetchWithTimeout(it.url, { method: "HEAD" });
                  if (!res.ok) res = await fetchWithTimeout(it.url, { method: "GET" });
                  if (!res.ok || res.status < 200 || res.status >= 300) return null;

                  const finalUrl = normalize(res.url || it.url);
                  if (!isLikelyArticle(finalUrl)) return null;

                  return {
                    title: String(it.title || "").trim(),
                    url: finalUrl,
                    source: it.source || "",
                    published_utc: new Date(it.published_utc).toISOString(),
                    ...(it.effects ? { effects: it.effects } : {}),
                    ...(it.category ? { category: it.category } : {}),
                    ...(it.region ? { region: it.region } : {})
                  };
                } catch {
                  return null;
                }
              })).filter(Boolean);

              // Prefer strict items, then top up with relaxed ones (dedup by URL)
              const seen = new Set(cleaned.map(x => x.url));
              for (const r of relaxed) {
                if (!seen.has(r.url)) {
                  cleaned.push(r);
                  seen.add(r.url);
                }
              }
            }

            // Save
            // (Keep wrapper if original was an object)
            saveJSON(kind, wrapper, cleaned);
            console.log(`Cleaner output ${cleaned.length} items`);
          })().catch(e => { console.error(e); process.exit(1); });
          EOF
          echo "Cleaner written to: $CLEANER"
          echo "CLEANER_PATH=$CLEANER" >> $GITHUB_ENV

      - name: Run cleaner (rewrite headlines.json)
        run: node "$CLEANER_PATH"

      - name: Post-clean probe (non-fatal log)
        shell: bash
        run: |
          set -euo pipefail
          echo "----- probe a sample of cleaned items -----"
          jq -r '.items[:20][] | .url' newsriver/headlines.json | while read -r URL; do
            [ -z "$URL" ] && continue
            CODE=$(curl -A "$MPB_ALT_UA" -L -m 15 -s -w '%{http_code}' -o /tmp/body.html "$URL" || echo "000")
            BYTES=$(wc -c </tmp/body.html | tr -d ' ')
            echo "$CODE $BYTES $URL"
          done

      - name: Commit & push with safe rebase
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add newsriver/headlines.json
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "ci: build+clean headlines.json (strict + relaxed fallback)"

          # Rebase and push (handle non-fast-forward & ephemeral conflicts on JSON)
          git fetch origin main
          if ! git pull --rebase origin main; then
            echo "Rebase conflict — keeping freshly generated headlines.json"
            git checkout --ours -- newsriver/headlines.json || true
            git add newsriver/headlines.json
            git rebase --continue || true
          fi

          for i in 1 2 3; do
            if git push origin HEAD:main; then
              echo "Push succeeded"
              break
            fi
            echo "Push failed (attempt $i) — rebasing again and retrying..."
            git fetch origin main
            if ! git pull --rebase origin main; then
              git checkout --ours -- newsriver/headlines.json || true
              git add newsriver/headlines.json
              git rebase --continue || true
            fi
          done
