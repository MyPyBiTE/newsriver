name: Build & publish headlines.json

on:
  workflow_dispatch:
  schedule:
    - cron: "*/12 * * * *"  # every 12 minutes

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    env:
      # Collector caps (collect plenty, then filter hard)
      MPB_MAX_PER_FEED: "14"
      MPB_MAX_TOTAL: "320"

      # Time + budgets
      MPB_HTTP_TIMEOUT: "10"
      MPB_SLOW_FEED_WARN: "3.5"
      MPB_GLOBAL_BUDGET: "210"
      NEWSRIVER_TIMEZONE: "America/Toronto"
      MPB_BREAKER_LIMIT: "3"

      # Recency + exact publish count
      MPB_MAX_AGE_HOURS: "69"
      MPB_MIN_AGE_SEC: "60"
      MPB_REQUIRE_EXACT_COUNT: "69"

      # Strict link verification + article heuristics
      MPB_VERIFY_LINKS: "1"
      MPB_REJECT_REDIRECT_TO_HOMEPAGE: "1"
      MPB_BLOCK_AGGREGATORS: "1"
      MPB_MIN_BODY_BYTES: "4096"
      MPB_MIN_ARTICLE_WORDS: "120"

      # Safety net
      MPB_FALLBACK_MIN_ITEMS: "1"
      MPB_FALLBACK_MAX_AGE_HOURS: "24"

      # Realistic UA
      MPB_ALT_UA: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"

      # Cleaner thresholds (post-build)
      CLEAN_MAX_AGE_HOURS: "72"         # hard clamp for ≤72h
      CLEAN_MIN_BODY_BYTES: "4096"      # soft-404/empty guard
      CLEAN_MIN_WORDS: "120"            # not an article if below
      CLEAN_CONCURRENCY: "6"
      CLEAN_TIMEOUT_MS: "12000"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install tools (jq + curl)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests beautifulsoup4 json5

      - name: Guardrail — strip embedded README block if present
        shell: bash
        run: |
          set -euo pipefail
          if grep -n '^readme = """' scripts/fetch_headlines.py >/dev/null 2>&1; then
            echo ">> Stripping embedded README block from scripts/fetch_headlines.py"
            sed -i '/^readme = """/,$d' scripts/fetch_headlines.py
          fi

      - name: Syntax check
        run: python -m py_compile scripts/fetch_headlines.py

      - name: Build headlines.json
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p newsriver
          python scripts/fetch_headlines.py --feeds-file feeds.txt --out newsriver/headlines.json
          echo "----- basic check"
          jq -r '.generated_utc, .count' newsriver/headlines.json || true
          echo "----- sample URLs"
          jq -r '.items[:10][] | [.published_utc, .source, .title, .url] | @tsv' newsriver/headlines.json || true

      - name: Set up Node (for cleaner)
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Write cleaner script (to temp, not repo)
        shell: bash
        run: |
          set -euo pipefail
          CLEANER="$RUNNER_TEMP/validate-links.mjs"
          cat << 'EOF' > "$CLEANER"
          // Post-build cleaner for newsriver/headlines.json
          // - keep only ≤72h items
          // - drop aggregators/home/section pages
          // - follow redirects; kill non-2xx
          // - detect soft-404/empty by body rules + keywords
          // - ensure "article-likeness" by URL and body (min words)
          // - preserve effects/category/region if present
          import fs from "node:fs";

          const DATA_PATH = "newsriver/headlines.json";

          const MAX_AGE_MS = Number(process.env.CLEAN_MAX_AGE_HOURS || "72") * 3600 * 1000;
          const MIN_BYTES  = Number(process.env.CLEAN_MIN_BODY_BYTES || "4096");
          const MIN_WORDS  = Number(process.env.CLEAN_MIN_WORDS || "120");
          const CONC       = Number(process.env.CLEAN_CONCURRENCY || "6");
          const TIMEOUT_MS = Number(process.env.CLEAN_TIMEOUT_MS || "12000");
          const UA         = process.env.MPB_ALT_UA || "Mozilla/5.0 mypybite-cleaner";

          const AGGR = /(news\.google|news\.yahoo|apple\.news|bing\.com\/news|msn\.com\/en-|flipboard\.com|drudgereport\.com|newsnow\.co\.uk|feedly\.com)/i;
          const SECTION = /\/(news|world|business|markets|sports|technology|tech|culture|entertainment|opinion|politics|video|videos|live|frontpage|homepage|home|today)\/*$/i;
          const HOMEPAGE = /^\/?($|[#?])/;
          const SOFT_404_PAT = /(404|not\s+found|page\s+not\s+found|no\s+longer\s+available|removed|error\s+404)/i;

          function absoluteUrl(u) {
            if (!u) return null;
            if (/^\/\//.test(u)) return "https:" + u;
            if (/^[a-z0-9.-]+\.[a-z]{2,}\b(\/|$)/i.test(u)) return "https://" + u;
            if (/^https?:\/\//i.test(u)) return u;
            return null;
          }

          function normalize(u) {
            try {
              const url = new URL(u);
              url.hash = "";
              const skip = /^(utm_|fbclid$|gclid$|mc_(cid|eid)$|ref$|cmpid$|source$|scid$)/i;
              const kept = [...url.searchParams.entries()].filter(([k]) => !skip.test(k));
              kept.sort(([a],[b]) => a.localeCompare(b));
              url.search = "";
              kept.forEach(([k,v]) => url.searchParams.append(k,v));
              return url.toString().replace(/\/+$/,'');
            } catch {
              return u;
            }
          }

          function isLikelyArticle(u) {
            try {
              const url = new URL(u);
              const p = url.pathname;
              if (!/^https?:\/\//i.test(u)) return false;
              if (AGGR.test(u)) return false;
              if (HOMEPAGE.test(p) || SECTION.test(p)) return false;
              const segs = p.split("/").filter(Boolean);
              if (segs.length <= 1 && !/\d{4}/.test(p)) return false;
              if (!/[a-z]{3,}/i.test(decodeURIComponent(p))) return false;
              return true;
            } catch {
              return false;
            }
          }

          function within72h(tsIso) {
            const ts = Date.parse(tsIso || "");
            return Number.isFinite(ts) && (Date.now() - ts <= MAX_AGE_MS);
          }

          async function fetchWithTimeout(url, opts = {}) {
            const ctl = new AbortController();
            const t = setTimeout(() => ctl.abort(), TIMEOUT_MS);
            try {
              const res = await fetch(url, {
                redirect: "follow",
                headers: { "user-agent": UA, "accept": "text/html,application/xhtml+xml" },
                signal: ctl.signal,
                ...opts,
              });
              clearTimeout(t);
              return res;
            } catch (e) {
              clearTimeout(t);
              return { ok:false, status:0, url, text: async () => "" };
            }
          }

          function looksLikeArticleBody(html = "") {
            if (!html || html.length < MIN_BYTES) return false;
            if (SOFT_404_PAT.test(html)) return false;
            const text = html
              .replace(/<script[\s\S]*?<\/script>/gi," ")
              .replace(/<style[\s\S]*?<\/style>/gi," ")
              .replace(/<[^>]+>/g," ")
              .replace(/\s+/g," ")
              .trim();
            const words = text.split(" ").filter(Boolean);
            return words.length >= MIN_WORDS;
          }

          function loadJSON() {
            const raw = JSON.parse(fs.readFileSync(DATA_PATH, "utf8"));
            if (Array.isArray(raw)) return { kind:"array", items: raw, wrapper:null };
            if (Array.isArray(raw?.items)) return { kind:"object", items: raw.items, wrapper: raw };
            return { kind:"array", items:[], wrapper:null };
          }

          function saveJSON(kind, wrapper, items) {
            if (kind === "object") {
              const out = {
                ...wrapper,
                generated_utc: new Date().toISOString(),
                count: items.length,
                items
              };
              fs.writeFileSync(DATA_PATH, JSON.stringify(out, null, 2) + "\n","utf8");
            } else {
              fs.writeFileSync(DATA_PATH, JSON.stringify(items, null, 2) + "\n","utf8");
            }
          }

          // simple concurrency limiter
          async function mapLimit(arr, limit, fn) {
            const out = new Array(arr.length);
            let i = 0, active = 0;
            return new Promise((resolve) => {
              const pump = () => {
                while (active < limit && i < arr.length) {
                  const idx = i++; active++;
                  Promise.resolve(fn(arr[idx], idx))
                    .then(v => { out[idx] = v; })
                    .catch(() => { out[idx] = null; })
                    .finally(() => { active--; pump(); });
                }
                if (i >= arr.length && active === 0) resolve(out);
              };
              pump();
            });
          }

          (async function main(){
            const { kind, wrapper, items } = loadJSON();

            // Pre-filter by age & URL shape
            const seen = new Set();
            const base = items.map(it => {
              const u = absoluteUrl(it?.canonical_url || it?.url) || absoluteUrl(it?.url);
              if (!u) return null;
              return {
                ...it,
                url: normalize(u),
                published_utc: it?.published_utc || it?.published || it?.date || new Date().toISOString()
              };
            }).filter(Boolean)
              .filter(it => within72h(it.published_utc))
              .filter(it => isLikelyArticle(it.url))
              .filter(it => { if (seen.has(it.url)) return false; seen.add(it.url); return true; });

            const cleaned = await mapLimit(base, CONC, async (it) => {
              try {
                // HEAD first (some sites block it; fallback to GET)
                let res = await fetchWithTimeout(it.url, { method: "HEAD" });
                if (!res.ok) res = await fetchWithTimeout(it.url, { method: "GET" });
                if (!res.ok || res.status < 200 || res.status >= 300) return null;

                const finalUrl = normalize(res.url || it.url);
                if (!isLikelyArticle(finalUrl)) return null;

                // Always GET body to detect soft-404/emptiness
                const res2 = await fetchWithTimeout(finalUrl, { method: "GET" });
                if (!res2.ok) return null;
                const body = await res2.text();
                if (!looksLikeArticleBody(body)) return null;

                const keep = {
                  title: String(it.title || "").trim(),
                  url: finalUrl,
                  source: it.source || "",
                  published_utc: new Date(it.published_utc).toISOString()
                };
                if (it.effects) keep.effects = it.effects;
                if (it.category) keep.category = it.category;
                if (it.region) keep.region = it.region;
                return keep;
              } catch {
                return null;
              }
            });

            const out = cleaned.filter(Boolean);
            saveJSON(kind, wrapper, out);
            console.log(`Cleaner kept ${out.length} / ${items.length} items`);
          })().catch(e => { console.error(e); process.exit(1); });
          EOF
          echo "Cleaner written to: $CLEANER"
          echo "CLEANER_PATH=$CLEANER" >> $GITHUB_ENV

      - name: Run cleaner (rewrite headlines.json)
        run: node "$CLEANER_PATH"

      - name: Post-clean probe (non-fatal log)
        shell: bash
        run: |
          set -euo pipefail
          echo "----- probe a sample of cleaned items -----"
          jq -r '.items[:20][] | .url' newsriver/headlines.json | while read -r URL; do
            [ -z "$URL" ] && continue
            CODE=$(curl -A "$MPB_ALT_UA" -L -m 15 -s -w '%{http_code}' -o /tmp/body.html "$URL" || echo "000")
            BYTES=$(wc -c </tmp/body.html | tr -d ' ')
            echo "$CODE $BYTES $URL"
          done

      - name: Commit & push with rebase (handles non-fast-forward)
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          # Only stage the generated JSON so the working tree remains clean
          git add newsriver/headlines.json
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi
          git commit -m "ci: build+clean headlines.json (drop 404/soft-404, ≤72h, non-articles)"
          # Ensure no untracked/unstaged changes exist before rebase
          git status --porcelain
          git fetch origin main
          git pull --rebase origin main
          git push origin HEAD:main
