name: Build & publish headlines.json

on:
  workflow_dispatch:
  schedule:
    - cron: "*/12 * * * *"  # every 12 minutes

# Prevent overlapping runs from racing each other.
# IMPORTANT: queue newer runs instead of canceling the in-progress one,
# so a long build isn’t killed by the next schedule tick.
concurrency:
  group: newsriver-headlines
  cancel-in-progress: false

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    env:
      # Collector caps (collect plenty, then filter hard)
      MPB_MAX_PER_FEED: "14"
      MPB_MAX_TOTAL: "320"

      # Time + budgets
      MPB_HTTP_TIMEOUT: "10"
      MPB_SLOW_FEED_WARN: "3.5"
      MPB_GLOBAL_BUDGET: "210"
      NEWSRIVER_TIMEZONE: "America/Toronto"
      MPB_BREAKER_LIMIT: "3"

      # Recency + exact publish count (pre-clean)
      MPB_MAX_AGE_HOURS: "69"
      MPB_MIN_AGE_SEC: "60"
      MPB_REQUIRE_EXACT_COUNT: "69"

      # Strict link verification + article heuristics (collector phase)
      MPB_VERIFY_LINKS: "1"
      MPB_REJECT_REDIRECT_TO_HOMEPAGE: "1"
      MPB_BLOCK_AGGREGATORS: "1"
      MPB_MIN_BODY_BYTES: "2048"     # (relaxed from 4096)
      MPB_MIN_ARTICLE_WORDS: "80"    # (relaxed from 120)

      # Safety net if collector is sparse
      MPB_FALLBACK_MIN_ITEMS: "1"
      MPB_FALLBACK_MAX_AGE_HOURS: "24"

      # Realistic UA
      MPB_ALT_UA: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"

      # Cleaner thresholds (post-build)
      CLEAN_MAX_AGE_HOURS: "72"         # hard clamp for ≤72h
      CLEAN_MIN_BODY_BYTES: "2048"      # (relaxed from 4096)
      CLEAN_MIN_WORDS: "80"             # (relaxed from 120)
      CLEAN_CONCURRENCY: "6"
      CLEAN_TIMEOUT_MS: "12000"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install tools (jq + curl)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests beautifulsoup4 json5

      - name: Ensure output dir
        run: |
          set -euo pipefail
          mkdir -p newsriver
          ls -lah newsriver || true

      - name: Guardrail — strip embedded README block if present
        shell: bash
        run: |
          set -euo pipefail
          if grep -n '^readme = """' scripts/fetch_headlines.py >/dev/null 2>&1; then
            echo ">> Stripping embedded README block from scripts/fetch_headlines.py"
            sed -i '/^readme = """/,$d' scripts/fetch_headlines.py
          fi

      - name: Syntax check
        run: python -m py_compile scripts/fetch_headlines.py

      - name: Build headlines.json
        shell: bash
        run: |
          set -euo pipefail
          python scripts/fetch_headlines.py --feeds-file feeds.txt --out newsriver/headlines.json
          echo "----- collector snapshot"
          jq -r '._debug.version, .generated_utc, .count' newsriver/headlines.json || true
          jq -r '.items[:6][] | [.published_utc, .source, .title, .url] | @tsv' newsriver/headlines.json || true

      - name: Set up Node (for cleaner)
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Write cleaner script (temp file)
        shell: bash
        run: |
          set -euo pipefail
          CLEANER="$RUNNER_TEMP/validate-links.mjs"
          cat << 'EOF' > "$CLEANER"
          import fs from "node:fs";

          const DATA_PATH = "newsriver/headlines.json";

          const MAX_AGE_MS = Number(process.env.CLEAN_MAX_AGE_HOURS || "72") * 3600 * 1000;
          const MIN_BYTES  = Number(process.env.CLEAN_MIN_BODY_BYTES || "2048");
          const MIN_WORDS  = Number(process.env.CLEAN_MIN_WORDS || "80");
          const CONC       = Number(process.env.CLEAN_CONCURRENCY || "6");
          const TIMEOUT_MS = Number(process.env.CLEAN_TIMEOUT_MS || "12000");
          const UA         = process.env.MPB_ALT_UA || "Mozilla/5.0 mypybite-cleaner";

          const TARGET_MIN_STRICT = 60;   // if strict clean < 60, try relaxed
          const TARGET_MIN_RELAX  = 20;   // if relaxed < 20, keep original

          const AGGR = /(news\.google|news\.yahoo|apple\.news|bing\.com\/news|msn\.com\/en-|flipboard\.com|drudgereport\.com|newsnow\.co\.uk|feedly\.com)/i;
          const SECTION = /\/(news|world|business|markets|sports|technology|tech|culture|entertainment|opinion|politics|video|videos|live|frontpage|homepage|home|today)\/*$/i;
          const HOMEPAGE = /^\/?($|[#?])/;
          const SOFT_404_PAT = /(404|not\s+found|page\s+not\s+found|no\s+longer\s+available|removed|error\s+404)/i;

          function absoluteUrl(u){
            if (!u) return null;
            if (/^\/\//.test(u)) return "https:" + u;
            if (/^[a-z0-9.-]+\.[a-z]{2,}\b(\/|$)/i.test(u)) return "https://" + u;
            if (/^https?:\/\//i.test(u)) return u;
            return null;
          }
          function normalize(u){
            try{
              const url = new URL(u); url.hash = "";
              const skip = /^(utm_|fbclid$|gclid$|mc_(cid|eid)$|ref$|cmpid$|source$|scid$)/i;
              const kept = [...url.searchParams.entries()].filter(([k]) => !skip.test(k)).sort(([a],[b]) => a.localeCompare(b));
              url.search = ""; kept.forEach(([k,v]) => url.searchParams.append(k,v));
              return url.toString().replace(/\/+$/,'');
            }catch{ return u; }
          }
          function isLikelyArticle(u){
            try{
              const url = new URL(u); const p = url.pathname;
              if (!/^https?:\/\//i.test(u)) return false;
              if (AGGR.test(u)) return false;
              if (HOMEPAGE.test(p) || SECTION.test(p)) return false;
              const segs = p.split("/").filter(Boolean);
              if (segs.length <= 1 && !/\d{4}/.test(p)) return false;
              if (!/[a-z]{3,}/i.test(decodeURIComponent(p))) return false;
              return true;
            }catch{ return false; }
          }
          function withinWindow(tsIso){
            const ts = Date.parse(tsIso || "");
            return Number.isFinite(ts) && (Date.now() - ts <= MAX_AGE_MS);
          }
          async function fetchWithTimeout(url, opts = {}){
            const ctl = new AbortController();
            const t = setTimeout(() => ctl.abort(), TIMEOUT_MS);
            try{
              const res = await fetch(url, {
                redirect: "follow",
                headers: { "user-agent": UA, "accept": "text/html,application/xhtml+xml" },
                signal: ctl.signal, ...opts
              });
              clearTimeout(t); return res;
            }catch{
              clearTimeout(t); return { ok:false, status:0, url, text: async () => "" };
            }
          }
          function looksLikeArticleBody(html = ""){
            if (!html || html.length < MIN_BYTES) return false;
            if (SOFT_404_PAT.test(html)) return false;
            const text = html.replace(/<script[\s\S]*?<\/script>/gi," ")
                             .replace(/<style[\s\S]*?<\/style>/gi," ")
                             .replace(/<[^>]+>/g," ").replace(/\s+/g," ").trim();
            const words = text.split(" ").filter(Boolean);
            return words.length >= MIN_WORDS;
          }
          function loadJSON(){
            const raw = JSON.parse(fs.readFileSync(DATA_PATH, "utf8"));
            if (Array.isArray(raw)) return { kind:"array", items: raw, wrapper:null };
            if (Array.isArray(raw?.items)) return { kind:"object", items: raw.items, wrapper: raw };
            return { kind:"array", items:[], wrapper:null };
          }
          function saveJSON(kind, wrapper, items){
            if (kind === "object"){
              const out = { ...wrapper, generated_utc: new Date().toISOString(), count: items.length, items };
              fs.writeFileSync(DATA_PATH, JSON.stringify(out, null, 2) + "\n","utf8");
            }else{
              fs.writeFileSync(DATA_PATH, JSON.stringify(items, null, 2) + "\n","utf8");
            }
          }
          async function mapLimit(arr, limit, fn){
            const out = new Array(arr.length); let i=0, active=0;
            return new Promise((resolve) => {
              const pump = () => {
                while (active < limit && i < arr.length){
                  const idx = i++; active++;
                  Promise.resolve(fn(arr[idx], idx))
                    .then(v => { out[idx]=v; })
                    .catch(() => { out[idx]=null; })
                    .finally(() => { active--; pump(); });
                }
                if (i >= arr.length && active === 0) resolve(out);
              };
              pump();
            });
          }

          function prefilter(items, relaxed = false){
            const seen = new Set();
            return items.map(it => {
              const u = absoluteUrl(it?.canonical_url || it?.url) || absoluteUrl(it?.url);
              if (!u) return null;
              const published_utc = it?.published_utc || it?.published || it?.date || new Date().toISOString();
              return { ...it, url: normalize(u), published_utc };
            }).filter(Boolean)
              .filter(it => withinWindow(it.published_utc))
              .filter(it => {
                // relaxed mode: allow section pages; strict mode: require article-likeness
                if (!relaxed && !isLikelyArticle(it.url)) return false;
                if (AGGR.test(it.url)) return false;
                if (seen.has(it.url)) return false; seen.add(it.url); return true;
              });
          }

          async function strictClean(items){
            const base = prefilter(items, false);
            const cleaned = await mapLimit(base, CONC, async (it) => {
              try{
                let res = await fetchWithTimeout(it.url, { method:"HEAD" });
                if (!res.ok) res = await fetchWithTimeout(it.url, { method:"GET" });
                if (!res.ok || res.status < 200 || res.status >= 300) return null;
                const finalUrl = normalize(res.url || it.url);
                if (!isLikelyArticle(finalUrl)) return null;
                const res2 = await fetchWithTimeout(finalUrl, { method: "GET" });
                if (!res2.ok) return null;
                const body = await res2.text();
                if (!looksLikeArticleBody(body)) return null;
                const keep = {
                  title: String(it.title || "").trim(),
                  url: finalUrl,
                  source: it.source || "",
                  published_utc: new Date(it.published_utc).toISOString()
                };
                if (it.effects) keep.effects = it.effects;
                if (it.category) keep.category = it.category;
                if (it.region) keep.region = it.region;
                return keep;
              }catch{ return null; }
            });
            return cleaned.filter(Boolean);
          }

          // relaxedClean: no body fetch; just age + not-aggregator + URL shape (looser)
          async function relaxedClean(items){
            return prefilter(items, true).map(it => {
              const keep = {
                title: String(it.title || "").trim(),
                url: it.url,
                source: it.source || "",
                published_utc: new Date(it.published_utc).toISOString()
              };
              if (it.effects) keep.effects = it.effects;
              if (it.category) keep.category = it.category;
              if (it.region) keep.region = it.region;
              return keep;
            });
          }

          (async function main(){
            const { kind, wrapper, items } = loadJSON();

            // Pass 1: strict
            let out = await strictClean(items);
            if (out.length < TARGET_MIN_STRICT){
              console.log(`Strict clean produced ${out.length} items; trying relaxed mode...`);
              const alt = await relaxedClean(items);
              if (alt.length > out.length){
                out = alt;
                console.log(`Relaxed mode kept ${out.length} items.`);
              }
            }

            // Final guard: never publish "empty" river
            if (out.length < TARGET_MIN_RELAX){
              console.log(`Cleaner would publish only ${out.length} items; keeping original collector output.`);
              // keep original wrapper shape if present
              if (kind === "object"){
                saveJSON(kind, wrapper, items);
              }else{
                saveJSON(kind, wrapper, items);
              }
              return;
            }

            // Save cleaned output
            saveJSON(kind, wrapper, out);
            console.log(`Cleaner kept ${out.length} / ${items.length} items`);
          })().catch(e => { console.error(e); process.exit(1); });
          EOF
          echo "CLEANER_PATH=$CLEANER" >> $GITHUB_ENV
          echo "Cleaner written: $CLEANER"

      - name: Run cleaner (rewrite headlines.json)
        run: node "$CLEANER_PATH"

      - name: Post-clean probe (log only)
        shell: bash
        run: |
          set -euo pipefail
          echo "----- probe a sample of cleaned items -----"
          jq -r '.items[:12][] | .url' newsriver/headlines.json | while read -r URL; do
            [ -z "$URL" ] && continue
            # Explicit no-cache headers to avoid stale CDN bodies in probe
            CODE=$(curl -A "$MPB_ALT_UA" -H 'Cache-Control: no-cache, no-store, max-age=0' -H 'Pragma: no-cache' -L -m 15 -s -w '%{http_code}' -o /tmp/body.html "$URL" || echo "000")
            BYTES=$(wc -c </tmp/body.html | tr -d ' ')
            echo "$CODE $BYTES $URL"
          done
          echo "----- summary"
          jq -r '._debug.version, .generated_utc, .count' newsriver/headlines.json || true

      - name: Commit & push with rebase (conflict-safe)
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Force-add in case .gitignore excludes JSON or folder
          git add -f newsriver/headlines.json

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "ci: build+clean headlines.json (relaxed thresholds + fallback)"

          git fetch origin main
          # Try fast-forward via rebase
          if ! git pull --rebase origin main; then
            echo "Rebase conflict – preferring generated headlines.json"
            git checkout --ours -- newsriver/headlines.json || true
            git add -f newsriver/headlines.json
            git rebase --continue || true
          fi

          # Push with up to 3 retries
          for i in 1 2 3; do
            if git push origin HEAD:main; then
              echo "Push succeeded"
              break
            fi
            echo "Push failed (attempt $i) – rebasing again and retrying..."
            git fetch origin main
            if ! git rebase origin/main; then
              git checkout --ours -- newsriver/headlines.json || true
              git add -f newsriver/headlines.json
              git rebase --continue || true
            fi
          done
